{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC, SVMSMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from pandas import DataFrame\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from configs import cfg\n",
    "from data.based.sampling_types import Sampling\n",
    "from data.based.transformers_enums import TransformersType\n",
    "\n",
    "#  Copyright (c) 2021, Omid Erfanmanesh, All rights reserved.\n",
    "\n",
    "seed = 2021\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "*BasedDataset* contains functions that are related to manipulate the dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BasedDataset:\n",
    "\n",
    "    def __init__(self, cfg, dataset_type):\n",
    "\n",
    "        self._cfg = cfg\n",
    "        self.dataset_type = dataset_type\n",
    "        self.dataset_address = cfg.DATASET.DATASET_ADDRESS\n",
    "        self.target_col = cfg.DATASET.TARGET\n",
    "        self.dataset_description_file = cfg.DATASET.DATASET_BRIEF_DESCRIPTION\n",
    "\n",
    "        if self.dataset_description_file is not None:\n",
    "            self.about = self.__open_txt_file(self.dataset_description_file)\n",
    "\n",
    "        self.load_dataset()\n",
    "        self.df = self.df_main.copy()\n",
    "        self.pca = None\n",
    "        self.encoded_data = None\n",
    "        self.scaled_data = None\n",
    "\n",
    "    def load_dataset(self):\n",
    "        \"\"\"\n",
    "        load dataset from csv file to dataframe\n",
    "        \"\"\"\n",
    "        if self.dataset_type == FileTypes.CSV:\n",
    "            self.df_main = self.__create_csv_dataframe()\n",
    "        else:\n",
    "            raise ValueError('dataset should be CSV file')\n",
    "\n",
    "    def drop_cols(self):\n",
    "        \"\"\"\n",
    "        drop columns from df\n",
    "\n",
    "        \"\"\"\n",
    "        if self._cfg.DATASET.DROP_COLS is not None:\n",
    "            cols = list(self._cfg.DATASET.DROP_COLS)\n",
    "            self.df = self.df.drop(labels=cols, axis=1)\n",
    "\n",
    "    def transformation(self, data: DataFrame, trans_type=None):\n",
    "        \"\"\"\n",
    "        change the distribution of data by using log transformation, ...\n",
    "\n",
    "        :param data:\n",
    "        :param trans_type:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            if trans_type is None:\n",
    "                cols = [*self._cfg.TRANSFORMATION]\n",
    "                cols = [x.lower() for x in cols]\n",
    "                cols = [x for x in cols if x in data.columns]\n",
    "                _min = data[cols].min()\n",
    "                for index, val in _min.iteritems():\n",
    "                    if val <= 0:\n",
    "                        data[index] = data[index] + 1 - val\n",
    "                for col in cols:\n",
    "                    if col in data.columns:\n",
    "                        trans_type = self._cfg.TRANSFORMATION[col.upper()]\n",
    "                        if trans_type == TransformersType.LOG:\n",
    "                            data[col] = np.log(data[col])\n",
    "                        elif trans_type == TransformersType.SQRT:\n",
    "                            data[col] = np.sqrt(data[col])\n",
    "                        elif trans_type == TransformersType.BOX_COX:\n",
    "                            data[col] = stats.boxcox(data[col])[0]\n",
    "            else:\n",
    "                _min = data.min()\n",
    "                for index, val in _min.iteritems():\n",
    "                    if val <= 0:\n",
    "                        data[index] = data[index] + 1 - val\n",
    "                if trans_type == TransformersType.LOG:\n",
    "                    data = np.log(data)\n",
    "                elif trans_type == TransformersType.SQRT:\n",
    "                    data = np.sqrt(data)\n",
    "                elif trans_type == TransformersType.BOX_COX:\n",
    "                    for col in data.columns:\n",
    "                        data[col] = stats.boxcox(data[col])[0]\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print('transform can not be applied ', e)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def categorical_features(self, data=None):\n",
    "        \"\"\"\n",
    "        select just categorical features from df\n",
    "\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if data is None:\n",
    "            return self.df.select_dtypes(include=['object']).columns.tolist()\n",
    "        else:\n",
    "            return data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    def numerical_features(self, data=None):\n",
    "        \"\"\"\n",
    "         select just numerical features from df\n",
    "\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if data is None:\n",
    "            return self.df.select_dtypes(exclude=['object']).columns.tolist()\n",
    "        else:\n",
    "            return data.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "    def select_columns(self, data, cols=None, just_numerical=False):\n",
    "        \"\"\"\n",
    "        select columns from df\n",
    "\n",
    "        :param data:\n",
    "        :param cols: array of columns that will be selected, None means select numerical features\n",
    "        :param just_numerical:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if cols is None or just_numerical:\n",
    "            cols = self.numerical_features(data=data)\n",
    "        return data[cols]\n",
    "\n",
    "    def split_to(self, test_size=0.10, val_size=0.10, has_validation=False, use_pca=False, random_state=seed):\n",
    "        \"\"\"\n",
    "        split dataset to train, test, validation set.\n",
    "\n",
    "        :param test_size: size of test set\n",
    "        :param val_size: size of validation set\n",
    "        :param has_validation: set True if validation set is required\n",
    "        :param use_pca: split dataset from pca components\n",
    "        :param random_state:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        _X, _y = self.__samples_and_labels(use_pca=use_pca)\n",
    "\n",
    "        _X_train, _X_test, _y_train, _y_test = train_test_split(_X, _y, test_size=test_size, random_state=random_state)\n",
    "        if has_validation:\n",
    "            _X_train, _X_val, _y_train, y_val = train_test_split(_X_train, _y_train, test_size=val_size,\n",
    "                                                                 random_state=random_state)\n",
    "            return _X_train, _X_val, _X_test, _y_train, y_val, _y_test\n",
    "        else:\n",
    "            return _X_train, _X_test, _y_train, _y_test\n",
    "\n",
    "    def generate_new_column_name(self, col, prefix):\n",
    "        \"\"\"\n",
    "        generate new name for columns\n",
    "\n",
    "        :param col:\n",
    "        :param prefix:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return '{}_{}'.format(col, prefix)\n",
    "\n",
    "    def __samples_and_labels(self, use_pca=False):\n",
    "        \"\"\"\n",
    "        return data as X and target values as y\n",
    "\n",
    "        :param use_pca: select X from pca\n",
    "        :return: data and label from df or pca\n",
    "        \"\"\"\n",
    "        _X = None\n",
    "        _y = None\n",
    "        if use_pca:\n",
    "            if self.pca is not None:\n",
    "                _X = self.pca.copy().drop(labels=[self.target_col], axis=1)\n",
    "                _y = self.pca[self.target_col].copy()\n",
    "            else:\n",
    "                print('pca data frame is not provided')\n",
    "        else:\n",
    "            _X = self.df.copy().drop(labels=[self.target_col], axis=1)\n",
    "            _y = self.df[self.target_col].copy()\n",
    "\n",
    "        return _X, _y\n",
    "\n",
    "    def resampling(self, X, y):\n",
    "        \"\"\"\n",
    "        resample dataset if you have imbalance data\n",
    "\n",
    "        :param X: data\n",
    "        :param y: targets\n",
    "        :return: return new data with resampling strategy\n",
    "        \"\"\"\n",
    "\n",
    "        if self._cfg.BASIC.SAMPLING_STRATEGY is None:\n",
    "            raise ValueError(\" SAMPLING_STRATEGY is None, Check the defaults.py\")\n",
    "\n",
    "        steps = []\n",
    "        if type(self._cfg.BASIC.SAMPLING_STRATEGY) is tuple:\n",
    "            sampling_types = [*self._cfg.BASIC.SAMPLING_STRATEGY]\n",
    "            for smp in sampling_types:\n",
    "                step = self.__resampling_pipeline(sampling_type=smp)\n",
    "                steps.append(step)\n",
    "        else:\n",
    "            sampling_types = self._cfg.BASIC.SAMPLING_STRATEGY\n",
    "            step = self.__resampling_pipeline(sampling_type=sampling_types)\n",
    "            steps.append(step)\n",
    "\n",
    "        pipeline = Pipeline(steps=steps)\n",
    "        X, y = pipeline.fit_resample(X, y)\n",
    "        return X, y\n",
    "\n",
    "    def __resampling_pipeline(self, sampling_type):\n",
    "        \"\"\"\n",
    "        create a pipeline for resampling data\n",
    "\n",
    "        :param sampling_type:\n",
    "        :return: a pipeline contains resampling strategies\n",
    "        \"\"\"\n",
    "        steps = None\n",
    "\n",
    "        if sampling_type == Sampling.RANDOM_UNDER_SAMPLING:\n",
    "\n",
    "            params = {\n",
    "                'sampling_strategy': self._cfg.RANDOM_UNDER_SAMPLER.SAMPLING_STRATEGY,\n",
    "                'random_state': self._cfg.RANDOM_UNDER_SAMPLER.RANDOM_STATE,\n",
    "                'replacement': self._cfg.RANDOM_UNDER_SAMPLER.REPLACEMENT\n",
    "            }\n",
    "            random_under_sampler = RandomUnderSampler(**params)\n",
    "            steps = ('random_under_sampler', random_under_sampler)\n",
    "        elif sampling_type == Sampling.RANDOM_OVER_SAMPLING:\n",
    "            params = {\n",
    "                'sampling_strategy': self._cfg.RANDOM_OVER_SAMPLER.SAMPLING_STRATEGY,\n",
    "                'random_state': self._cfg.RANDOM_OVER_SAMPLER.RANDOM_STATE,\n",
    "                # 'shrinkage': self._cfg.RANDOM_OVER_SAMPLER.SHRINKAGE\n",
    "            }\n",
    "            random_over_sampler = RandomOverSampler(**params)\n",
    "            steps = ('random_over_sampler', random_over_sampler)\n",
    "        elif sampling_type == Sampling.SMOTE:\n",
    "            params = {\n",
    "                'sampling_strategy': self._cfg.SMOTE.SAMPLING_STRATEGY,\n",
    "                'random_state': self._cfg.SMOTE.RANDOM_STATE,\n",
    "                'k_neighbors': self._cfg.SMOTE.K_NEIGHBORS,\n",
    "                'n_jobs': self._cfg.SMOTE.N_JOBS\n",
    "            }\n",
    "            smote = SMOTE(**params)\n",
    "            steps = ('smote', smote)\n",
    "        elif sampling_type == Sampling.SMOTENC:\n",
    "            params = {\n",
    "                'categorical_features': self._cfg.SMOTENC.CATEGORICAL_FEATURES,\n",
    "                'sampling_strategy': self._cfg.SMOTENC.SAMPLING_STRATEGY,\n",
    "                'random_state': self._cfg.SMOTENC.RANDOM_STATE,\n",
    "                'k_neighbors': self._cfg.SMOTENC.K_NEIGHBORS,\n",
    "                'n_jobs': self._cfg.SMOTENC.N_JOBS\n",
    "            }\n",
    "            smotenc = SMOTENC(**params)\n",
    "            steps = ('smotenc', smotenc)\n",
    "        elif sampling_type == Sampling.SVMSMOTE:\n",
    "            params = {\n",
    "                'sampling_strategy': self._cfg.SVMSMOTE.SAMPLING_STRATEGY,\n",
    "                'random_state': self._cfg.SVMSMOTE.RANDOM_STATE,\n",
    "                'k_neighbors': self._cfg.SVMSMOTE.K_NEIGHBORS,\n",
    "                'n_jobs': self._cfg.SVMSMOTE.N_JOBS,\n",
    "                'm_neighbors': self._cfg.SVMSMOTE.M_NEIGHBORS,\n",
    "                # 'svm_estimator': self._cfg.SMOTE.SVM_ESTIMATOR,\n",
    "                'out_step': self._cfg.SVMSMOTE.OUT_STEP\n",
    "            }\n",
    "            svmsmote = SVMSMOTE(**params)\n",
    "            steps = ('svmsmote', svmsmote)\n",
    "\n",
    "        return steps\n",
    "\n",
    "    def __create_csv_dataframe(self):\n",
    "        \"\"\"\n",
    "        read data from csv file\n",
    "        :return: pandas dataframe\n",
    "        \"\"\"\n",
    "        return pd.read_csv(self.dataset_address, delimiter=';')\n",
    "\n",
    "    def __open_txt_file(self, desc):\n",
    "        \"\"\"\n",
    "        read contents from txt file\n",
    "        :param desc:\n",
    "        :return: contents as text\n",
    "        \"\"\"\n",
    "        return open(desc, 'r').read()\n",
    "\n",
    "    @property\n",
    "    def df(self):\n",
    "        return self._df\n",
    "\n",
    "    @df.setter\n",
    "    def df(self, df: DataFrame):\n",
    "        self._df = df\n",
    "\n",
    "    @property\n",
    "    def pca(self):\n",
    "        return self._pca\n",
    "\n",
    "    @pca.setter\n",
    "    def pca(self, value):\n",
    "        self._pca = value\n",
    "\n",
    "    @property\n",
    "    def df_main(self):\n",
    "        return self._df_main\n",
    "\n",
    "    @df_main.setter\n",
    "    def df_main(self, value):\n",
    "        self._df_main = value\n",
    "\n",
    "    @property\n",
    "    def dataset_address(self):\n",
    "        return self._dataset_address\n",
    "\n",
    "    @dataset_address.setter\n",
    "    def dataset_address(self, address):\n",
    "        self._dataset_address = address\n",
    "\n",
    "    @property\n",
    "    def dataset_type(self):\n",
    "        return self._dataset_type\n",
    "\n",
    "    @dataset_type.setter\n",
    "    def dataset_type(self, value):\n",
    "        self._dataset_type = value\n",
    "\n",
    "    @property\n",
    "    def target_col(self):\n",
    "        return self._target_col\n",
    "\n",
    "    @target_col.setter\n",
    "    def target_col(self, target):\n",
    "        self._target_col = target\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self.df_main[self.target_col]\n",
    "\n",
    "    @property\n",
    "    def about(self):\n",
    "        return self._about\n",
    "\n",
    "    @about.setter\n",
    "    def about(self, about):\n",
    "        self._about = about\n",
    "\n",
    "    @property\n",
    "    def dataset_description_file(self):\n",
    "        return self._dataset_description_file\n",
    "\n",
    "    @dataset_description_file.setter\n",
    "    def dataset_description_file(self, value):\n",
    "        self._dataset_description_file = value\n",
    "\n",
    "    @property\n",
    "    def encoded_data(self):\n",
    "        return self._encoded_data\n",
    "\n",
    "    @encoded_data.setter\n",
    "    def encoded_data(self, value):\n",
    "        self._encoded_data = value\n",
    "\n",
    "    @property\n",
    "    def scaled_data(self):\n",
    "        return self._scaled_data\n",
    "\n",
    "    @scaled_data.setter\n",
    "    def scaled_data(self, value):\n",
    "        self._scaled_data = value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Bank* is a sub-class of *BasedDataset*. It contains functions that are related to manipulate the dataset.\n",
    "The functions are named equaly to each column of csv file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from data.based.file_types import FileTypes\n",
    "\n",
    "\n",
    "class Bank(BasedDataset):\n",
    "    def __init__(self, cfg):\n",
    "        super(Bank, self).__init__(cfg=cfg, dataset_type=FileTypes.CSV)\n",
    "\n",
    "    def age(self):\n",
    "        self.df.loc[self.df['age'] <= 32, 'age'] = 1\n",
    "        self.df.loc[(self.df['age'] > 32) & (self.df['age'] <= 47), 'age'] = 2\n",
    "        self.df.loc[(self.df['age'] > 47) & (self.df['age'] <= 70), 'age'] = 3\n",
    "        self.df.loc[(self.df['age'] > 70) & (self.df['age'] <= 98), 'age'] = 4\n",
    "\n",
    "    def job(self):\n",
    "        pass\n",
    "\n",
    "    def marital(self):\n",
    "        pass\n",
    "\n",
    "    def education(self):\n",
    "        pass\n",
    "\n",
    "    def default(self):\n",
    "        pass\n",
    "\n",
    "    def balance(self):\n",
    "        pass\n",
    "\n",
    "    def housing(self):\n",
    "        pass\n",
    "\n",
    "    def loan(self):\n",
    "        pass\n",
    "\n",
    "    def contact(self):\n",
    "        pass\n",
    "\n",
    "    def day(self):\n",
    "        pass\n",
    "\n",
    "    def month(self):\n",
    "        pass\n",
    "\n",
    "    def duration(self):\n",
    "        self.df.loc[self.df['duration'] <= 102, 'duration'] = 1\n",
    "        self.df.loc[(self.df['duration'] > 102) & (self.df['duration'] <= 180), 'duration'] = 2\n",
    "        self.df.loc[(self.df['duration'] > 180) & (self.df['duration'] <= 319), 'duration'] = 3\n",
    "        self.df.loc[(self.df['duration'] > 319) & (self.df['duration'] <= 644.5), 'duration'] = 4\n",
    "        self.df.loc[self.df['duration'] > 644.5, 'duration'] = 5\n",
    "\n",
    "    def campaign(self):\n",
    "        pass\n",
    "\n",
    "    def pdays(self):\n",
    "        pass\n",
    "\n",
    "    def previous(self):\n",
    "        pass\n",
    "\n",
    "    def poutcome(self):\n",
    "        self.df['poutcome'].replace(['nonexistent', 'failure', 'success'], [1, 2, 3], inplace=True)\n",
    "\n",
    "    def y(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def load(cfg):\n",
    "    bank = Bank(cfg=cfg)\n",
    "    return bank\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To apply principal component analysis, use *PCA* Class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "#  Copyright (c) 2021, Omid Erfanmanesh, All rights reserved.\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA as skl_pca\n",
    "\n",
    "sns.set()\n",
    "\n",
    "\n",
    "class PCA:\n",
    "    def __init__(self, cfg):\n",
    "        self.n_components = cfg.PCA.N_COMPONENTS\n",
    "        self.pca = skl_pca(n_components=self.n_components, random_state=cfg.BASIC.RAND_STATE)\n",
    "\n",
    "    def apply(self, data=None, y=None,\n",
    "              X_train=None, X_test=None):\n",
    "        \"\"\"\n",
    "        apply pca to data\n",
    "        :param y:\n",
    "        :param X_test:\n",
    "        :param X_train:\n",
    "        :param data:\n",
    "        :return: dataframe of pca components\n",
    "        \"\"\"\n",
    "        if data is not None and y is not None:\n",
    "            _components = self.pca.fit_transform(data)\n",
    "            print('Explained variance: %.4f' % self.pca.explained_variance_ratio_.sum())\n",
    "            print('Individual variance contributions:')\n",
    "            for j in range(self.pca.n_components_):\n",
    "                print(f\"PC({j}): {self.pca.explained_variance_ratio_[j]}\")\n",
    "\n",
    "            _columns = ['pc' + str(i + 1) for i in range(self.pca.n_components_)]\n",
    "            _columns.append('y')\n",
    "\n",
    "            y = np.reshape(y.values, (y.values.shape[0], -1)).copy()\n",
    "            _components = np.concatenate((_components, y), axis=1)\n",
    "            _pca_df = pd.DataFrame(data=_components\n",
    "                                   , columns=_columns)\n",
    "\n",
    "            return _pca_df\n",
    "\n",
    "        elif X_train is not None and X_test is not None:\n",
    "            _components = self.pca.fit(X=X_train)\n",
    "            print('Explained variance: %.4f' % self.pca.explained_variance_ratio_.sum())\n",
    "            print('Individual variance contributions:')\n",
    "            for j in range(self.pca.n_components_):\n",
    "                print(f\"PC({j}): {self.pca.explained_variance_ratio_[j]:.4f}\")\n",
    "            _columns = ['pc' + str(i + 1) for i in range(self.pca.n_components_)]\n",
    "\n",
    "            n_train = self.pca.transform(X=X_train)\n",
    "            n_test = self.pca.transform(X=X_test)\n",
    "\n",
    "            df_train = pd.DataFrame(data=n_train, columns=_columns)\n",
    "            df_test = pd.DataFrame(data=n_test, columns=_columns)\n",
    "\n",
    "            return df_train, df_test\n",
    "\n",
    "    def plot(self, X, y):\n",
    "        \"\"\"\n",
    "        scatter plot of pca components\n",
    "        :param X:\n",
    "        :param y:\n",
    "        \"\"\"\n",
    "        X['y'] = y\n",
    "        sns.pairplot(X, hue=\"y\", height=2.5)\n",
    "        plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Scaler* Class provides functions that are needed to change scale of data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, RobustScaler, StandardScaler\n",
    "\n",
    "from data.based.scale_types import ScaleTypes\n",
    "\n",
    "\n",
    "class Scalers:\n",
    "    def __init__(self, cfg):\n",
    "        self._cfg = cfg\n",
    "\n",
    "    def __get_scaler(self, scale_type):\n",
    "        \"\"\"\n",
    "        get scaler object\n",
    "        :param scale_type:\n",
    "        :return: scaler object, name of scaler\n",
    "        \"\"\"\n",
    "        scl = None\n",
    "        scl_name = None\n",
    "        if scale_type == ScaleTypes.MIN_MAX:\n",
    "            scl = MinMaxScaler()\n",
    "            scl_name = 'min_max_scaler'\n",
    "        elif scale_type == ScaleTypes.STANDARD:\n",
    "            scl = StandardScaler()\n",
    "            scl_name = 'min_max_scaler'\n",
    "        elif scale_type == ScaleTypes.MAX_ABS:\n",
    "            scl = MaxAbsScaler()\n",
    "            scl_name = 'max_abs_scaler'\n",
    "        elif scale_type == ScaleTypes.ROBUST:\n",
    "            scl = RobustScaler()\n",
    "            scl_name = 'robust_scaler'\n",
    "\n",
    "        return scl, scl_name\n",
    "\n",
    "    def do_scale(self, data=None, X_train=None, X_test=None):\n",
    "        \"\"\"\n",
    "        get data and apply scaler function\n",
    "\n",
    "        :param data: change scale of whole data\n",
    "        :param X_train: change scale of train set and fit scaler to it\n",
    "        :param X_test: transform test data scale based on train set\n",
    "        :return: scaled dataframe\n",
    "        \"\"\"\n",
    "        if data is None:\n",
    "            _train_scale, _test_scale = self.__get_scaled_values(X_train=X_train, X_test=X_test)\n",
    "            _train_df = pd.DataFrame(data=_train_scale, columns=X_train.columns)\n",
    "            _test_df = pd.DataFrame(data=_test_scale, columns=X_train.columns)\n",
    "            return _train_df, _test_df\n",
    "        else:\n",
    "            _data_scale = self.__get_scaled_values(data=data)\n",
    "            _data_df = pd.DataFrame(data=_data_scale, columns=data.columns)\n",
    "            return _data_df\n",
    "\n",
    "    def __get_scaled_values(self, data=None, X_train=None, X_test=None):\n",
    "        \"\"\"\n",
    "        get scaler configs and objects and apply scaler\n",
    "        :param data:\n",
    "        :param X_train:\n",
    "        :param X_test:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        scl_type = self._cfg.SCALER\n",
    "        scl, scl_name = self.__get_scaler(scale_type=scl_type)\n",
    "        if data is None:\n",
    "            return self.__apply(scl=scl, X_train=X_train, X_test=X_test)\n",
    "        else:\n",
    "            return self.__apply(scl=scl, data=data)\n",
    "\n",
    "    def __apply(self, scl, data=None, X_train=None, X_test=None):\n",
    "        \"\"\"\n",
    "        apply scaler to data\n",
    "        :param scl: scaler obj\n",
    "        :param data: data\n",
    "        :param X_train: train set\n",
    "        :param X_test: test set\n",
    "        :return: scaled data\n",
    "        \"\"\"\n",
    "        if data is None:\n",
    "            scl.fit(X_train)\n",
    "            train_scale = scl.transform(X_train)\n",
    "            test_scale = None\n",
    "            if X_test is not None:\n",
    "                test_scale = scl.transform(X_test)\n",
    "            return train_scale, test_scale\n",
    "        else:\n",
    "            scl.fit(data)\n",
    "            train_scale = scl.transform(data)\n",
    "            return train_scale"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can encode your categorical features by using *Encoder* Class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "\n",
    "from category_encoders import OneHotEncoder, OrdinalEncoder, BinaryEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data.based.encoder_enum import EncoderTypes\n",
    "\n",
    "\n",
    "class Encoders:\n",
    "    def __init__(self, cdg):\n",
    "        self._cfg = cdg\n",
    "\n",
    "    def __get_encoder(self, encoder_type, col):\n",
    "        \"\"\"\n",
    "        initialize encoder object\n",
    "\n",
    "        :param encoder_type:\n",
    "        :param col: columns that you want to encode.Iit is used for category_encoders package\n",
    "        :return: encoder obj, encoder name\n",
    "        \"\"\"\n",
    "        le = None\n",
    "        le_name = None\n",
    "        if encoder_type == EncoderTypes.LABEL:\n",
    "            le = LabelEncoder()\n",
    "            le_name = 'label_encoding'\n",
    "        elif encoder_type == EncoderTypes.ORDINAL:\n",
    "            le = OrdinalEncoder(cols=[col])\n",
    "            le_name = 'ordinal_encoding'\n",
    "        elif encoder_type == EncoderTypes.ONE_HOT:\n",
    "            le = OneHotEncoder(cols=[col])\n",
    "            le_name = 'one_hot_encoding'\n",
    "        elif encoder_type == EncoderTypes.BINARY:\n",
    "            le = BinaryEncoder(cols=[col])\n",
    "            le_name = 'binary_encoding'\n",
    "\n",
    "        return le, le_name\n",
    "\n",
    "    def __get_encoded_data(self, enc, data, y, X_train=None, X_test=None, y_train=None, y_test=None):\n",
    "        \"\"\"\n",
    "        returned encoded data\n",
    "\n",
    "        :param enc:\n",
    "        :param data:\n",
    "        :param y:\n",
    "        :param X_train:\n",
    "        :param X_test:\n",
    "        :param y_train:\n",
    "        :param y_test:\n",
    "        :return: encoded dataframe\n",
    "        \"\"\"\n",
    "        train_enc, test_enc, data_enc = None, None, None\n",
    "        if isinstance(enc, LabelEncoder):\n",
    "            if data is None and y is None:\n",
    "                enc.fit(X_train)\n",
    "                train_enc = enc.transform(X_train)\n",
    "                test_enc = None\n",
    "                if X_test is not None:\n",
    "                    test_enc = enc.transform(X_test)\n",
    "            else:\n",
    "                enc.fit(data)\n",
    "                data_enc = enc.transform(data)\n",
    "        else:\n",
    "            if data is None and y is None:\n",
    "                enc.fit(X_train, y_train)\n",
    "                train_enc = enc.transform(X_train, y_train)\n",
    "                test_enc = None\n",
    "                if X_test is not None:\n",
    "                    test_enc = enc.transform(X_test, y_test)\n",
    "            else:\n",
    "                enc.fit(data, y)\n",
    "                data_enc = enc.transform(data, y)\n",
    "        if data is None and y is None:\n",
    "            return train_enc, test_enc\n",
    "        else:\n",
    "            return data_enc\n",
    "\n",
    "    def __encode_by_configs(self, data=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None):\n",
    "        \"\"\"\n",
    "        encode data and set the configurations\n",
    "        :param data:\n",
    "        :param y:\n",
    "        :param X_train:\n",
    "        :param X_test:\n",
    "        :param y_train:\n",
    "        :param y_test:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for col in tqdm(self._cfg.ENCODER):\n",
    "            encode_type = self._cfg.ENCODER[col]\n",
    "            col = col.lower()\n",
    "            if col == self._cfg.DATASET.TARGET:\n",
    "                continue\n",
    "            if X_train is not None and col not in X_train.columns:\n",
    "                continue\n",
    "            if data is not None and col not in data.columns:\n",
    "                continue\n",
    "            enc, enc_name = self.__get_encoder(encoder_type=encode_type, col=col)\n",
    "            if encode_type == EncoderTypes.LABEL:\n",
    "                if data is None and y is None:\n",
    "                    train_val = X_train[col].values\n",
    "                    test_val = X_test[col].values\n",
    "\n",
    "                    X_train[col], X_test[col] = self.__get_encoded_data(enc=enc, data=None, y=None, X_train=train_val,\n",
    "                                                                        X_test=test_val)\n",
    "                else:\n",
    "                    train_val = data[col].values\n",
    "                    data[col] = self.__get_encoded_data(enc=enc, data=train_val, y=y)\n",
    "            else:\n",
    "                if data is None and y is None:\n",
    "                    X_train, X_test = self.__get_encoded_data(enc=enc, data=None, y=None, X_train=X_train,\n",
    "                                                              X_test=X_test,\n",
    "                                                              y_train=y_train, y_test=y_test)\n",
    "                else:\n",
    "                    data = self.__get_encoded_data(enc=enc, data=data, y=y)\n",
    "\n",
    "        if data is None and y is None:\n",
    "            return X_train, X_test\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "    def do_encode(self, data=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None):\n",
    "        \"\"\"\n",
    "        apply encoders\n",
    "        :param data:\n",
    "        :param y:\n",
    "        :param X_train:\n",
    "        :param X_test:\n",
    "        :param y_train:\n",
    "        :param y_test:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'data': data,\n",
    "            'y': y,\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test\n",
    "        }\n",
    "\n",
    "        return self.__encode_by_configs(**params)\n",
    "\n",
    "    def custom_encoding(self, data, col, encode_type):\n",
    "        \"\"\"\n",
    "        if you want to use custom encoders for a specific column\n",
    "\n",
    "        :param data:\n",
    "        :param col:\n",
    "        :param encode_type:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        enc, enc_name = self.__get_encoder(encoder_type=encode_type, col=col)\n",
    "        return enc.fit_transform(data[col])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All functions that you need to train your model will be provided in *BasedModel* Class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dtreeviz.trees import dtreeviz  # remember to load the package\n",
    "# Metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.callbacks import DeadlineStopper, VerboseCallback\n",
    "\n",
    "from model.based.metric_types import MetricTypes\n",
    "from model.based.tuning_mode import TuningMode\n",
    "\n",
    "\n",
    "class BasedModel:\n",
    "    def __init__(self, cfg):\n",
    "        self.model = None\n",
    "        self._metric_function = cfg.EVALUATION.METRIC\n",
    "        self._fold = cfg.MODEL.K_FOLD\n",
    "        self.name = None\n",
    "        self.use_for_feature_importance = False\n",
    "        self.fine_tune_params = {}\n",
    "        self._confusion_matrix = cfg.EVALUATION.CONFUSION_MATRIX\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        train the model \n",
    "        \n",
    "        :param X_train: \n",
    "        :param y_train: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        print('start training...')\n",
    "        self.model.fit(X_train, y_train)\n",
    "        return self.model\n",
    "\n",
    "    def evaluate(self, X_test, y_test, target_labels=None, normalize=None):\n",
    "        \"\"\"\n",
    "        evaluate the model based on a metric \n",
    "        :param X_test: test set\n",
    "        :param y_test: test targets\n",
    "        :param target_labels: distinct target values in list \n",
    "        :param normalize: it is for confusion matrix\n",
    "        \"\"\"\n",
    "        print('evaluation...')\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        score = self.metric(y_test, y_pred)\n",
    "        print(f'score is {score}')\n",
    "\n",
    "        if self._confusion_matrix:\n",
    "            plot_confusion_matrix(self.model, X_test, y_test, cmap=plt.cm.Blues,\n",
    "                                  display_labels=target_labels,\n",
    "                                  normalize=normalize)\n",
    "            plt.show()\n",
    "\n",
    "    def metric(self, y_true=None, y_pred=None):\n",
    "        \"\"\"\n",
    "        initialize the metric for evaluation the model\n",
    "        \n",
    "        :param y_true: \n",
    "        :param y_pred: \n",
    "        :return: metric obj or name\n",
    "        \"\"\"\n",
    "        metric_type = self._metric_function\n",
    "\n",
    "        if y_pred is None and y_true is None:\n",
    "            if metric_type == MetricTypes.F1_SCORE_BINARY:\n",
    "                return 'f1'\n",
    "            elif metric_type == MetricTypes.F1_SCORE_MICRO:\n",
    "                return 'f1_micro'\n",
    "            elif metric_type == MetricTypes.F1_SCORE_MACRO:\n",
    "                return 'f1_macro'\n",
    "            elif metric_type == MetricTypes.F1_SCORE_WEIGHTED:\n",
    "                return 'f1_weighted'\n",
    "            elif metric_type == MetricTypes.F1_SCORE_SAMPLE:\n",
    "                return 'f1_samples'\n",
    "            elif metric_type == MetricTypes.PRECISION:\n",
    "                return 'precision'\n",
    "            elif metric_type == MetricTypes.RECALL:\n",
    "                return 'recall'\n",
    "            elif metric_type == MetricTypes.ACCURACY:\n",
    "                return 'accuracy'\n",
    "        else:\n",
    "            if metric_type == MetricTypes.F1_SCORE_BINARY:\n",
    "                return f1_score(y_true, y_pred, average=\"binary\")\n",
    "            elif metric_type == MetricTypes.F1_SCORE_MICRO:\n",
    "                return f1_score(y_true, y_pred, average=\"micro\")\n",
    "            elif metric_type == MetricTypes.F1_SCORE_MACRO:\n",
    "                return f1_score(y_true, y_pred, average=\"macro\")\n",
    "            elif metric_type == MetricTypes.F1_SCORE_WEIGHTED:\n",
    "                return f1_score(y_true, y_pred, average=\"weighted\")\n",
    "            elif metric_type == MetricTypes.F1_SCORE_SAMPLE:\n",
    "                return f1_score(y_true, y_pred, average=\"sample\")\n",
    "            elif metric_type == MetricTypes.PRECISION:\n",
    "                return precision_score(y_true, y_pred)\n",
    "            elif metric_type == MetricTypes.RECALL:\n",
    "                return recall_score(y_true, y_pred)\n",
    "            elif metric_type == MetricTypes.ACCURACY:\n",
    "                return accuracy_score(y_true, y_pred)\n",
    "\n",
    "    def hyper_parameter_tuning(self, X, y, title='', method=TuningMode.GRID_SEARCH):\n",
    "        \"\"\"\n",
    "        apply hyper parameter tuning \n",
    "        :param X: \n",
    "        :param y: \n",
    "        :param title: \n",
    "        :param method: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        opt = None\n",
    "        callbacks = None\n",
    "        if self.fine_tune_params:\n",
    "            if method == TuningMode.GRID_SEARCH:\n",
    "                opt = GridSearchCV(estimator=self.model, param_grid=self.fine_tune_params, cv=3, n_jobs=-1, verbose=3,\n",
    "                                   scoring=self.metric())\n",
    "            elif method == TuningMode.BAYES_SEARCH:\n",
    "                opt = BayesSearchCV(self.model, self.fine_tune_params)\n",
    "                callbacks = [VerboseCallback(100), DeadlineStopper(60 * 10)]\n",
    "\n",
    "            best_params = self.report_best_params(optimizer=opt, X=X, y=y, title=title,\n",
    "                                                  callbacks=callbacks)\n",
    "            return best_params\n",
    "        else:\n",
    "            print('There are no params for tuning')\n",
    "\n",
    "    def feature_importance(self, features=None):\n",
    "        \"\"\"\n",
    "        detect important features for a model\n",
    "        \n",
    "        :param features: column names, it will be used for printing the columns\n",
    "        \"\"\"\n",
    "        if self.use_for_feature_importance:\n",
    "            if hasattr(self.model, 'coef_'):\n",
    "                importance = self.model.coef_[0]\n",
    "            else:\n",
    "                importance = self.model.feature_importances_\n",
    "            importance = np.array(importance).reshape((-1, 1))\n",
    "\n",
    "            # summarize feature importance\n",
    "            for i, v in enumerate(importance):\n",
    "                if features:\n",
    "                    print('Feature(%0d): %0s, Score: %.5f' % (i, str(features[i]), v))\n",
    "                else:\n",
    "                    print('Feature(%0s): Score: %.5f' % (str(i), v))\n",
    "\n",
    "            fs = pd.DataFrame(data=importance.T, columns=features)\n",
    "            ax = sns.barplot(data=fs)\n",
    "            plt.gcf().set_size_inches(11, 9)\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.title(f'Feature importance by using the model of {self.name}')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\" The model of {self.name} can not be used for estimating the importance of features\")\n",
    "\n",
    "    def report_best_params(self, optimizer, X, y, title, callbacks=None):\n",
    "        \"\"\"\n",
    "        A wrapper for measuring time and performances of different optimizers\n",
    "\n",
    "        optimizer = a sklearn or a skopt optimizer\n",
    "        X = the training set\n",
    "        y = our target\n",
    "        title = a string label for the experiment\n",
    "        \"\"\"\n",
    "        start = time()\n",
    "        if callbacks:\n",
    "            optimizer.fit(X, y, callback=callbacks)\n",
    "        else:\n",
    "            optimizer.fit(X, y)\n",
    "\n",
    "        d = pd.DataFrame(optimizer.cv_results_)\n",
    "        best_score = optimizer.best_score_\n",
    "        best_score_std = d.iloc[optimizer.best_index_].std_test_score\n",
    "        best_params = optimizer.best_params_\n",
    "        if best_params:\n",
    "            print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n",
    "                   + u\"\\u00B1\" + \" %.3f\") % (time() - start,\n",
    "                                             len(optimizer.cv_results_['params']),\n",
    "                                             best_score,\n",
    "                                             best_score_std))\n",
    "            print('Best parameters:')\n",
    "            pprint(best_params)\n",
    "            print()\n",
    "        else:\n",
    "            print('There are no params provided')\n",
    "\n",
    "        return best_params\n",
    "\n",
    "    def plot_tree(self, X, y, target_name, feature_names, class_names):\n",
    "        \"\"\"\n",
    "        plot the decision trees. Note that it will be work just for decision tree classifier \n",
    "        :param X: \n",
    "        :param y: \n",
    "        :param target_name: \n",
    "        :param feature_names: \n",
    "        :param class_names: \n",
    "        \"\"\"\n",
    "        viz = dtreeviz(self.model, X, y,\n",
    "                       target_name=target_name,\n",
    "                       feature_names=feature_names,\n",
    "                       class_names=list(class_names))\n",
    "        viz.save(\"decision_tree.svg\")\n",
    "        viz.view()\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "\n",
    "    @model.setter\n",
    "    def model(self, value):\n",
    "        self._model = value\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    @name.setter\n",
    "    def name(self, value):\n",
    "        self._name = value\n",
    "\n",
    "    @property\n",
    "    def fine_tune_params(self):\n",
    "        return self._fine_tune_params\n",
    "\n",
    "    @fine_tune_params.setter\n",
    "    def fine_tune_params(self, value):\n",
    "        self._fine_tune_params = value\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "\n",
    "    @model.setter\n",
    "    def model(self, value):\n",
    "        self._model = value\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:37: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:63: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:37: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:63: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-25-156c800c5890>:37: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if _k is 'C':\n",
      "<ipython-input-25-156c800c5890>:63: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if _k is 'C':\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "from model.based import TaskMode\n",
    "\n",
    "\n",
    "class SVM(BasedModel):\n",
    "    def __init__(self, cfg):\n",
    "        super(SVM, self).__init__(cfg=cfg)\n",
    "        self._task_mode = cfg.BASIC.TASK_MODE\n",
    "\n",
    "        if self._task_mode == TaskMode.CLASSIFICATION:\n",
    "            self._params = {\n",
    "                'C': cfg.SVM.C,\n",
    "                'kernel': cfg.SVM.KERNEL,\n",
    "                'degree': cfg.SVM.DEGREE,\n",
    "                'gamma': cfg.SVM.GAMMA,\n",
    "                'coef0': cfg.SVM.COEF0,\n",
    "                'shrinking': cfg.SVM.SHRINKING,\n",
    "                'probability': cfg.SVM.PROBABILITY,\n",
    "                'tol': cfg.SVM.TOL,\n",
    "                'cache_size': cfg.SVM.CACHE_SIZE,\n",
    "                'class_weight': cfg.SVM.CLASS_WEIGHT,\n",
    "                'verbose': cfg.SVM.VERBOSE,\n",
    "                'max_iter': cfg.SVM.MAX_ITER,\n",
    "                'decision_function_shape': cfg.SVM.DECISION_FUNCTION_SHAPE,\n",
    "                'break_ties': cfg.SVM.BREAK_TIES,\n",
    "                'random_state': cfg.SVM.RANDOM_STATE\n",
    "\n",
    "            }\n",
    "            self.model = SVC(**self._params)\n",
    "            self.name = cfg.SVM.NAME\n",
    "            for _k in cfg.SVM.HYPER_PARAM_TUNING:\n",
    "                _param = cfg.SVM.HYPER_PARAM_TUNING[_k]\n",
    "\n",
    "                if _param is not None:\n",
    "                    _param = [*_param]\n",
    "                    if _k is 'C':\n",
    "                        self.fine_tune_params[_k] = [*_param]\n",
    "                    else:\n",
    "                        self.fine_tune_params[_k.lower()] = [*_param]\n",
    "\n",
    "        elif self._task_mode == TaskMode.REGRESSION:\n",
    "            self._params = {\n",
    "                'kernel': cfg.SVR.KERNEL,\n",
    "                'degree': cfg.SVR.DEGREE,\n",
    "                'gamma': cfg.SVR.GAMMA,\n",
    "                'coef0': cfg.SVR.COEF0,\n",
    "                'tol': cfg.SVR.TOL,\n",
    "                'C': cfg.SVR.C,\n",
    "                'epsilon': cfg.SVR.EPSILON,\n",
    "                'shrinking': cfg.SVR.SHRINKING,\n",
    "                'cache_size': cfg.SVR.CACHE_SIZE,\n",
    "                'verbose': cfg.SVR.VERBOSE,\n",
    "                'max_iter': cfg.SVR.MAX_ITER\n",
    "\n",
    "            }\n",
    "            self.model = SVR(**self._params)\n",
    "            self.name = cfg.SVR.NAME\n",
    "            for _k in cfg.SVR.HYPER_PARAM_TUNING:\n",
    "                _param = cfg.SVR.HYPER_PARAM_TUNING[_k]\n",
    "                if _param is not None:\n",
    "                    _param = [*_param]\n",
    "                    if _k is 'C':\n",
    "                        self.fine_tune_params[_k] = [*_param]\n",
    "                    else:\n",
    "                        self.fine_tune_params[_k.lower()] = [*_param]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: PYDEVD_USE_CYTHON environment variable is set to 'NO'. Frame evaluator will be also disabled because it requires Cython extensions to be enabled in order to operate correctly.\n"
     ]
    }
   ],
   "source": [
    "#  Copyright (c) 2021, Omid Erfanmanesh, All rights reserved.\n",
    "\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from data.based.based_dataset import BasedDataset\n",
    "from data.preprocessing import Encoders, Scalers, PCA\n",
    "from model.based import BasedModel\n",
    "from model.based.tuning_mode import TuningMode\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "def do_train(cfg, model: BasedModel, dataset: BasedDataset, encoder: Encoders, scaler: Scalers, pca: PCA,\n",
    "             feature_importance=False):\n",
    "    # encode target values\n",
    "    dataset.df[dataset.target_col] = encoder.custom_encoding(dataset.df, col=cfg.DATASET.TARGET,\n",
    "                                                             encode_type=cfg.ENCODER.Y)\n",
    "\n",
    "    if cfg.BASIC.TRANSFORMATION:\n",
    "        dataset.df = dataset.transformation(copy.deepcopy(dataset.df))\n",
    "\n",
    "    # split data to train and test sub-dataset\n",
    "    X_train, X_test, y_train, y_test = dataset.split_to()\n",
    "\n",
    "    # convert categorical features to integer\n",
    "    if encoder is None:\n",
    "        # select integer columns ( if your encoder is None, it will select just integer columns for training)\n",
    "        X_train = dataset.select_columns(data=X_train)\n",
    "        X_test = dataset.select_columns(data=X_test)\n",
    "    else:\n",
    "        X_train, X_test = encoder.do_encode(X_train=X_train, X_test=X_test, y_train=y_train,\n",
    "                                            y_test=y_test)\n",
    "\n",
    "    # change the scale of data\n",
    "    if scaler is not None:\n",
    "        X_train, X_test = scaler.do_scale(X_train=X_train, X_test=X_test)\n",
    "\n",
    "    if pca is not None:\n",
    "        # apply pca analysis to train set\n",
    "        X_train, X_test = pca.apply(X_train=X_train, X_test=X_test)\n",
    "        if cfg.PCA.PLOT:\n",
    "            pca.plot(X=copy.deepcopy(X_train), y=y_train)\n",
    "\n",
    "    # get columns before scaling data, it will be used for feature importance method\n",
    "    columns = X_train.columns\n",
    "\n",
    "    # if you set the resampling strategy, it will balance your data based on your strategy\n",
    "    if cfg.BASIC.SAMPLING_STRATEGY is not None:\n",
    "        counter = Counter(y_train)\n",
    "        print(f\"Before sampling {counter}\")\n",
    "        X_train, y_train = dataset.resampling(X=X_train, y=y_train)\n",
    "        counter = Counter(y_train)\n",
    "        print(f\"After sampling {counter}\")\n",
    "\n",
    "    # train the model\n",
    "    model.train(X_train=X_train, y_train=y_train)\n",
    "    # target unique values for plotting confusion matrix\n",
    "    class_names = dataset.df_main[dataset.target_col].unique()\n",
    "    # evaluate the model\n",
    "    model.evaluate(X_test=X_test, y_test=y_test, target_labels=class_names)\n",
    "\n",
    "    # plot important features\n",
    "    if feature_importance:\n",
    "        model.feature_importance(features=list(columns))\n",
    "\n",
    "    # # plot trees, it will work just for Decision Tree classifier\n",
    "    # model.plot_tree(X=X_train, y=y_train, target_name='y', feature_names=X_train.columns, class_names=class_names)\n",
    "\n",
    "\n",
    "def do_cross_val(cfg, model: BasedModel, dataset: BasedDataset, encoder: Encoders, scaler: Scalers, pca: PCA):\n",
    "    # encode target values\n",
    "    _y = encoder.custom_encoding(dataset.df, col=cfg.DATASET.TARGET,\n",
    "                                 encode_type=cfg.ENCODER.Y)\n",
    "    _X = dataset.df.drop(dataset.target_col, axis=1)\n",
    "\n",
    "    if cfg.BASIC.TRANSFORMATION:\n",
    "        _X = dataset.transformation(copy.deepcopy(_X))\n",
    "\n",
    "    if encoder is None:\n",
    "        _X = dataset.select_columns(data=_X)\n",
    "    else:\n",
    "        # convert categorical features to integer\n",
    "        _X = encoder.do_encode(data=_X, y=_y)\n",
    "\n",
    "    # change the scale of data\n",
    "    if scaler is not None:\n",
    "        _X = scaler.do_scale(data=_X)\n",
    "\n",
    "    # if you set the resampling strategy, it will balance your data based on your strategy\n",
    "    if cfg.BASIC.SAMPLING_STRATEGY is not None:\n",
    "        counter = Counter(_y)\n",
    "        print(f\"Before sampling {counter}\")\n",
    "        _X, _y = dataset.resampling(X=_X, y=_y)\n",
    "        counter = Counter(_y)\n",
    "        print(f\"After sampling {counter}\")\n",
    "\n",
    "    # config cross validation settings\n",
    "    cv = KFold(n_splits=cfg.MODEL.K_FOLD, random_state=cfg.BASIC.RAND_STATE, shuffle=cfg.MODEL.SHUFFLE)\n",
    "    # select the metric\n",
    "    metric = model.metric()\n",
    "    # get scores from cross validation\n",
    "    scores = cross_val_score(model.model, _X.values, _y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "    for s in scores:\n",
    "        print(f'{metric} is {s:.2f}')\n",
    "    print(f\"mean of {metric}: {np.mean(scores):.2f}\")\n",
    "\n",
    "\n",
    "def do_fine_tune(cfg, model: BasedModel, dataset: BasedDataset, encoder: Encoders, scaler: Scalers,\n",
    "                 method=TuningMode.GRID_SEARCH):\n",
    "    # split data to train and test sub-dataset\n",
    "    _X_train, _X_val, _X_test, _y_train, _y_val, _y_test = dataset.split_to(has_validation=True)\n",
    "\n",
    "    if encoder is None:\n",
    "        # select integer columns ( if your encoder is None, it will select just integer columns for training)\n",
    "        _X_train = dataset.select_columns(data=_X_train)\n",
    "        _X_val = dataset.select_columns(data=_X_val)\n",
    "    else:\n",
    "        # convert categorical features to integer\n",
    "        _X_train, _X_val = encoder.do_encode(X_train=_X_train, X_test=_X_val, y_train=_y_train,\n",
    "                                             y_test=_y_val)\n",
    "    # change the scale of data\n",
    "    _X_train, _X_val = scaler.do_scale(X_train=_X_train, X_test=_X_val)\n",
    "    # run tuning\n",
    "    model.hyper_parameter_tuning(X=_X_train, y=_y_train, title=model.name, method=method)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load dataset from csv file and manipulate columns such as *age* and *duration*\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look insde *cfg* file (this is out configuration file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(cfg['BASIC'])\n",
    "print('------------')\n",
    "print(cfg['DATASET'])\n",
    "print('------------')\n",
    "print(cfg['MODEL'])\n",
    "print('------------')\n",
    "print(cfg['EVALUATION'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: 0\n",
      "PCA: False\n",
      "RAND_STATE: 2021\n",
      "RUNTIME_MODE: 0\n",
      "SAMPLING_STRATEGY: None\n",
      "SEED: 2021\n",
      "TASK_MODE: 0\n",
      "TRANSFORMATION: False\n",
      "------------\n",
      "DATASET_ADDRESS: ../data/dataset/bank.csv\n",
      "DATASET_BRIEF_DESCRIPTION: ../data/dataset/description.txt\n",
      "DROP_COLS: ('day', 'balance', 'month', 'job', 'previous', 'campaign', 'education', 'pdays', 'marital', 'contact', 'housing', 'loan')\n",
      "HAS_CATEGORICAL_TARGETS: True\n",
      "TARGET: y\n",
      "------------\n",
      "K_FOLD: 5\n",
      "NUM_CLASSES: 2\n",
      "SHUFFLE: True\n",
      "------------\n",
      "CONFUSION_MATRIX: False\n",
      "METRIC: 1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "bank = load(cfg)  # create dataset object instance\n",
    "bank.load_dataset()  # load data from csv file\n",
    "bank.age()  # convert the range of the age's values to [1,2,3,4]\n",
    "bank.duration()  # convert the range of the duration's values to [1,2,3,4,5]\n",
    "bank.drop_cols()  # drop columns\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "model = SVM(cfg=cfg)\n",
    "encoder = Encoders(cdg=cfg)  # initialize Encoder object\n",
    "scaler = Scalers(cfg=cfg)  # initialize scaler object\n",
    "pca = None\n",
    "if cfg.BASIC.PCA:  # PCA object will be initialized if you set pca = True in configs file\n",
    "    pca = PCA(cfg=cfg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training SVM model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:00<00:00, 479.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "[LibSVM]evaluation...\n",
      "score is 0.8852097130242825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_train(cfg=cfg, dataset=bank, model=copy.deepcopy(model), encoder=encoder, scaler=scaler, pca=pca)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross-Validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:00<00:00, 530.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_micro is 0.90\n",
      "f1_micro is 0.89\n",
      "f1_micro is 0.88\n",
      "f1_micro is 0.90\n",
      "f1_micro is 0.90\n",
      "mean of f1_micro: 0.89\n"
     ]
    }
   ],
   "source": [
    "do_cross_val(cfg=cfg, dataset=bank, model=copy.deepcopy(model), encoder=encoder, scaler=scaler, pca=pca)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter Tuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# do_fine_tune(cfg=cfg, dataset=bank, model=copy.deepcopy(model), encoder=encoder, scaler=scaler,\n",
    "#                 method=TuningMode.GRID_SEARCH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}